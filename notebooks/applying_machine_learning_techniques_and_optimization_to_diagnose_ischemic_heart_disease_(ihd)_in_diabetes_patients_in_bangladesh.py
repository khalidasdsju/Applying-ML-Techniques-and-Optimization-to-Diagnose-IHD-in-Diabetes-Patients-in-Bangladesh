# -*- coding: utf-8 -*-
"""Applying Machine Learning Techniques and Optimization to Diagnose Ischemic Heart Disease (IHD) in Diabetes Patients in Bangladesh.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mmxxf8JAuPOpBvqdmZj13WdCvwp5Gn8G

# Applying Machine Learning Techniques and Optimization to Diagnose Ischemic Heart Disease (IHD) in Diabetes Patients in Bangladesh: Investigations from a Cross-Sectional Study in 2024

## Setting up my  environment for machine learning with some common libraries.
"""

# General Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# Scikit-learn Libraries
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, log_loss, precision_recall_curve
from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.metrics import roc_auc_score
from sklearn.linear_model import LogisticRegression, RidgeClassifier, LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from lightgbm import LGBMClassifier
from sklearn.svm import SVC
import lightgbm as lgb
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.multioutput import ClassifierChain
from sklearn.feature_selection import RFE, mutual_info_classif
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import label_binarize
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.linear_model import Ridge

# SciPy Libraries
from scipy.stats import fisher_exact
from scipy.stats.mstats import winsorize

# Statsmodels Library
from statsmodels.stats.contingency_tables import mcnemar
### Ignore Warnings
import warnings
warnings.filterwarnings("ignore")

"""### Data Preprocessing

##### Load the Data Set
"""

# Load the SPSS file
data = pd.read_spss('ASDS project Data.sav')

data.info()

"""##### Drop unnecessary columns"""

# Drop unnecessary columns
data =data.drop(columns=['Serial_No','Name'])

"""##### Covert Name"""

data.columns

# New column names should match the order of the old columns
# Old column names
old_columns = [
    'Age', 'Sex','Occupation','Education','Economy ','Hight','weight',
    'Systolic_Upper','Diastolic_Lower','RBS','Smoking','HTN','DM','Dyslipidemia','Stroke','IHD','Age_group',
    'BMI_Group','Hypertenson_stage']

new_columns_reordered = [
    'Age (Years)', 'Sex (Male/Female)', 'Occupation Type', 'Education Level', 'Economic Status',
    'Height (cm)', 'Weight (kg)', 'Systolic Blood Pressure (mmHg)', 'Diastolic Blood Pressure (mmHg)',
    'Random Blood Sugar (mg/dL)', 'Smoking Status', 'Hypertension (HTN) Status',
    'Diabetes Mellitus (DM) Status', 'Dyslipidemia Status', 'Stroke Status', 'Ischemic Heart Disease (IHD) Status',
    'Age Group', 'Body Mass Index (BMI) Group', 'Hypertension Stage']



# Check if the lengths match
print(f"Old Columns: {len(old_columns)}, New Columns: {len(new_columns_reordered)}")

# Assuming the DataFrame is called data
data.columns = new_columns_reordered

"""##### Cheak data Shape"""

data.shape

"""##### Cheak missing values"""

# Check for missing values
missing_values = data.isnull().sum()

# Display columns with missing values
print("Missing Values in Each Column:")
print(missing_values[missing_values > 0])

# Display the total number of missing values
total_missing = data.isnull().sum().sum()
print(f"\nTotal Missing Values in the Dataset: {total_missing}")

"""##### Handle Missing Values"""

data['Random Blood Sugar (mg/dL)'].fillna(data['Random Blood Sugar (mg/dL)'].median(), inplace=True)

"""##### Cheaking duplicate columns"""

# Check for duplicate columns
duplicate_columns = data.columns[data.columns.duplicated()].unique()

# Display the duplicate columns (if any)
if len(duplicate_columns) > 0:
    print(f"Duplicate columns found: {duplicate_columns}")
else:
    print("No duplicate columns found.")

"""##### Check for duplicate rows"""

# Check for duplicate rows
duplicate_rows = data[data.duplicated()]

# Display the duplicate rows (if any)
if not duplicate_rows.empty:
    print("Duplicate rows found:")
    print(duplicate_rows)
else:
    print("No duplicate rows found.")

"""## Outliners decetion and Removal

#### Plot Oulters
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Set up the plotting grid
num_cols = data.select_dtypes(include=[np.number]).columns
fig, axes = plt.subplots(nrows=len(num_cols), ncols=2, figsize=(10, len(num_cols)*5))

# Loop through numeric columns and plot histograms and scatter plots
for i, col in enumerate(num_cols):
    # Histogram
    sns.histplot(data[col], kde=True, ax=axes[i, 0])
    axes[i, 0].set_title(f'Histogram of {col}')

    # Detect outliers using IQR method
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identify outliers
    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]

    # Scatter plot with outliers in a different color
    sns.scatterplot(x=data.index, y=data[col], ax=axes[i, 1], color=['skyblue'])  # Normal points
    sns.scatterplot(x=outliers.index, y=outliers[col], ax=axes[i, 1], color='red')  # Outliers in red
    axes[i, 1].set_title(f'Scatter Plot of {col} with Outliers')

plt.tight_layout()
plt.show()

"""##### Remove outliers"""

# IQR-based outlier detection
def detect_outliers_iqr(data):
    numerical_data = data.select_dtypes(include=[np.number])
    Q1 = numerical_data.quantile(0.25)
    Q3 = numerical_data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identify outliers
    outliers = ((numerical_data < lower_bound) | (numerical_data > upper_bound)).sum()
    return outliers

# IQR-based outlier detection and replacing with median
def impute_outliers_with_median(data, max_iterations=5):
    numerical_data = data.select_dtypes(include=[np.number])
    for iteration in range(max_iterations):
        Q1 = numerical_data.quantile(0.25)
        Q3 = numerical_data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Identify and replace outliers with the median for each column
        for column in numerical_data.columns:
            outliers = (numerical_data[column] < lower_bound[column]) | (numerical_data[column] > upper_bound[column])
            if outliers.sum() > 0:
                median_value = numerical_data[column].median()
                numerical_data[column] = numerical_data[column].where(~outliers, median_value)

        # After replacing outliers, check if there are any outliers left
        outliers_after = detect_outliers_iqr(numerical_data)
        if all(outliers_after == 0):
            print(f"Outliers have been handled after {iteration + 1} iterations.")
            break
    # Return the modified dataset with outliers replaced by the median
    data[numerical_data.columns] = numerical_data
    return data

# Check outliers in the original dataset
outliers_before = detect_outliers_iqr(data)
print("Outliers detected before imputation:")
print(outliers_before)

# Apply the iterative imputation method
data_imputed = impute_outliers_with_median(data)

# Check outliers after imputation
outliers_after = detect_outliers_iqr(data_imputed)
print("\nOutliers detected after iterative median imputation:")
print(outliers_after)

# Verify if there are any outliers (should be 0 for each column)
if all(outliers_after == 0):
    print("\nNo outliers detected after iterative imputation.")
else:
    print("\nThere are still outliers present.")

"""##### If there are still outliers present ,remove Again"""

# Apply Winsorization to cap the extreme 5% values on both sides
data['Random Blood Sugar (mg/dL)'] = winsorize(data['Random Blood Sugar (mg/dL)'], limits=(0.05, 0.05))

# Check for outliers after Winsorization
outliers_after_winsorization = detect_outliers_iqr(data)

print("\nOutliers detected after Winsorization:")
print(outliers_after_winsorization)

# Verify if there are any outliers left in the "Random Blood Sugar (mg/dL)" column
if outliers_after_winsorization['Random Blood Sugar (mg/dL)'] == 0:
    print("\nAll outliers in 'Random Blood Sugar (mg/dL)' have been handled.")
else:
    print("\nThere are still outliers present in 'Random Blood Sugar (mg/dL)'.")

"""## Descriptive Statistics

##### Summery of Statistical Numberical Colums
"""

data.describe(include=[np.number]).T

data.info()

"""##### Summery of Statistical catagorical Colums"""

data.describe(include=['category','object']).T

"""##### Plotting for categorical variables"""

# Set up the plotting for categorical variables
cat_cols = data.select_dtypes(include=[object, 'category']).columns
fig_cat, axes_cat = plt.subplots(nrows=len(cat_cols), ncols=1, figsize=(10, len(cat_cols)*5))

# Loop through categorical columns and plot bar plots
for i, col in enumerate(cat_cols):
    ax = axes_cat[i] if len(cat_cols) > 1 else axes_cat

    # Countplot with multiple colors
    sns.countplot(x=data[col], ax=ax, palette=['skyblue','lightcoral','red','olive'])  # You can change the palette to any you prefer

    # Title for the plot
    ax.set_title(f'Bar Plot of {col}')

    # Calculate percentages
    total = len(data[col])
    for p in ax.patches:
        height = p.get_height()
        percentage = (height / total) * 100
        ax.text(p.get_x() + p.get_width() / 2, height + 1, f'{percentage:.2f}%', ha='center', va='bottom')

plt.tight_layout()
plt.show()

"""##### Plotting for neumerical variables"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Set up the plotting grid
num_cols = data.select_dtypes(include=[np.number]).columns
fig, axes = plt.subplots(nrows=len(num_cols), ncols=2, figsize=(10, len(num_cols)*5))

# Loop through numeric columns and plot histograms and scatter plots
for i, col in enumerate(num_cols):
    # Histogram
    sns.histplot(data[col], kde=True, ax=axes[i, 0])
    axes[i, 0].set_title(f'Histogram of {col}')

    # Detect outliers using IQR method
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identify outliers
    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]

    # Scatter plot with outliers in a different color
    sns.scatterplot(x=data.index, y=data[col], ax=axes[i, 1], color=['skyblue'])  # Normal points
    sns.scatterplot(x=outliers.index, y=outliers[col], ax=axes[i, 1], color='red')  # Outliers in red
    axes[i, 1].set_title(f'Scatter Plot of {col} with Outliers')

plt.tight_layout()
plt.show()

"""### Define the terget Variable

##### Distribution of terget Variable Ischemic Heart Disease (IHD) Status
"""

data['Ischemic Heart Disease (IHD) Status'].unique()

# Data
IHD_counts = data['Ischemic Heart Disease (IHD) Status'].value_counts()
IHD_percentage = IHD_counts / IHD_counts.sum() * 100

# Pie Chart with Shadows (Simulated 3D Effect)
plt.figure(figsize=(6, 4))
colors = ['skyblue','lightcoral','red','olive'][:len(IHD_counts)]

plt.pie(
    IHD_counts,
    labels=IHD_counts.index,
    autopct='%1.1f%%',
    startangle=90,
    colors=colors,
    explode=[0.1] * len(IHD_counts),  # Slightly separate each slice
    shadow=True  # Add shadow for depth
)
plt.title('Ischemic Heart Disease (IHD) Status')
plt.axis('equal')  # Ensures the pie chart is circular
plt.show()

# Bar Chart with Enhancements
fig, ax = plt.subplots(figsize=(5, 4))
colors = sns.color_palette("cool", len(IHD_counts))  # Gradient color palette

bars = ax.bar(IHD_counts.index, IHD_counts.values, color=['skyblue','lightcoral','red','olive'], edgecolor='white', linewidth=1.2)

# Annotate bars
for bar, percentage in zip(bars, IHD_percentage):
    ax.text(
        bar.get_x() + bar.get_width() / 2,
        bar.get_height() + 0.5,
        f'{int(bar.get_height())} ({percentage:.1f}%)',
        ha='center',
        va='bottom',
        fontsize=10
    )

ax.set_title('Ischemic Heart Disease (IHD) Status', fontsize=14, weight='bold')
ax.set_ylabel('Count', fontsize=12)
ax.set_xlabel('Ischemic Heart Disease (IHD) Status', fontsize=12)
plt.xticks(rotation=0, fontsize=10)
sns.despine()  # Remove top and right spines for a cleaner look
plt.show()

# Define categorical and numerical columns
categorical_cols = [
    'Sex (Male/Female)', 'Occupation Type', 'Education Level',
    'Economic Status', 'Smoking Status', 'Hypertension (HTN) Status',
    'Diabetes Mellitus (DM) Status', 'Dyslipidemia Status', 'Stroke Status',
    'Ischemic Heart Disease (IHD) Status', 'Age Group',
    'Body Mass Index (BMI) Group', 'Hypertension Stage'
]

numerical_cols = ['Age (Years)', 'Height (cm)', 'Weight (kg)',
       'Systolic Blood Pressure (mmHg)', 'Diastolic Blood Pressure (mmHg)',
       'Random Blood Sugar (mg/dL)']  # Example numerical columns

# Set a stylish background with a light gradient and muted gridlines
sns.set_theme(style="whitegrid", palette="muted", font_scale=1.2)
plt.rcParams['axes.facecolor'] = '#f9f9f9'  # Light background for axes
plt.rcParams['figure.facecolor'] = '#eaeaea'  # Subtle background for the figure
plt.rcParams['grid.color'] = '#d3d3d3'  # Softer gridline colors
plt.rcParams['axes.edgecolor'] = '#a1a1a1'  # Slightly darker edges for contrast

# Create subplots
num_plots = len(categorical_cols) + len(numerical_cols)
fig, axes = plt.subplots(
    nrows=num_plots,
    ncols=1,
    figsize=(12, num_plots * 4),  # Adjusted figsize calculation
    constrained_layout=True
)

# Plot categorical features with percentages
for idx, col in enumerate(categorical_cols):
    ax = axes[idx]
    sns.countplot(
        data=data,
        x=col,
        hue='Ischemic Heart Disease (IHD) Status',
        palette="coolwarm",  # Stylish blue-red color palette
        ax=ax,
        dodge=True,
        width=0.6  # Slimmer bars
    )

    # Add percentage annotations
    total = len(data)
    for p in ax.patches:
        height = p.get_height()
        if height > 0:  # Avoid division by zero
            percentage = (height / total) * 100
            ax.annotate(
                f'{percentage:.0f}%',  # Rounded percentage
                (p.get_x() + p.get_width() / 2., height),
                ha='center', va='bottom', fontsize=9, color='#333333'
            )

    # Customize the plot
    ax.set_title(f'{col} vs Ischemic Heart Disease (IHD)', fontsize=14, weight='bold')
    ax.set_xlabel(col, fontsize=12)
    ax.set_ylabel('Count', fontsize=12)
    ax.legend(title='Ischemic Heart Disease (IHD)', fontsize=10)
    ax.tick_params(axis='x', rotation=30)

# Plot numerical features as boxplots
for idx, col in enumerate(numerical_cols, start=len(categorical_cols)):
    ax = axes[idx]
    sns.boxplot(data=data, x='Ischemic Heart Disease (IHD) Status', y=col, palette="coolwarm", ax=ax, width=0.5)  # Slimmer boxes

    # Customize the plot
    ax.set_title(f'{col} vs Ischemic Heart Disease (IHD)', fontsize=14, weight='bold')
    ax.set_xlabel('Ischemic Heart Disease', fontsize=12)
    ax.set_ylabel(col, fontsize=12)
    ax.grid(axis='y', linestyle='--', alpha=0.6)

# Final adjustments
fig.suptitle('Ischemic Heart Disease (IHD) Status Analysis', fontsize=16, weight='bold')
plt.show()

"""##### Diabetes Mellitus (DM) Status"""

data['Diabetes Mellitus (DM) Status'].unique()

import matplotlib.pyplot as plt

# Data
DM_counts = data['Diabetes Mellitus (DM) Status'].value_counts()
DM_percentage = DM_counts / DM_counts.sum() * 100

# Pie Chart with Shadows (Simulated 3D Effect)
plt.figure(figsize=(8, 6))
colors = ['skyblue','lightcoral','red','olive'][:len(IHD_counts)]

plt.pie(
    DM_counts,
    labels=DM_counts.index,
    autopct='%1.1f%%',
    startangle=90,
    colors=colors,
    explode=[0.1] * len(DM_counts),  # Slightly separate each slice
    shadow=True  # Add shadow for depth
)
plt.title('Diabetes Mellitus (DM) Status - Simulated 3D Pie Chart')
plt.axis('equal')  # Ensures the pie chart is circular
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Bar Chart with Enhancements
fig, ax = plt.subplots(figsize=(4, 6))
colors = sns.color_palette("cool", len(DM_counts))  # Gradient color palette

bars = ax.bar(DM_counts.index, DM_counts.values, color=['skyblue','lightcoral','red','olive'], edgecolor='white', linewidth=1.2)

# Annotate bars
for bar, percentage in zip(bars, DM_percentage):
    ax.text(
        bar.get_x() + bar.get_width() / 2,
        bar.get_height() + 0.5,
        f'{int(bar.get_height())} ({percentage:.1f}%)',
        ha='center',
        va='bottom',
        fontsize=10
    )

ax.set_title('Diabetes Mellitus (DM) Status- Enhanced Bar Chart', fontsize=14, weight='bold')
ax.set_ylabel('Count', fontsize=12)
ax.set_xlabel('Diabetes Mellitus (DM) Status', fontsize=12)
plt.xticks(rotation=0, fontsize=10)
sns.despine()  # Remove top and right spines for a cleaner look
plt.show()

# Define categorical and numerical columns
categorical_cols = [
    'Sex (Male/Female)', 'Occupation Type', 'Education Level',
    'Economic Status', 'Smoking Status', 'Hypertension (HTN) Status',
    'Diabetes Mellitus (DM) Status', 'Dyslipidemia Status', 'Stroke Status',
    'Ischemic Heart Disease (IHD) Status', 'Age Group',
    'Body Mass Index (BMI) Group', 'Hypertension Stage'
]

numerical_cols = ['Age (Years)', 'Height (cm)', 'Weight (kg)',
       'Systolic Blood Pressure (mmHg)', 'Diastolic Blood Pressure (mmHg)',
       'Random Blood Sugar (mg/dL)']  # Example numerical columns

# Set a stylish background with a light gradient and muted gridlines
sns.set_theme(style="whitegrid", palette="muted", font_scale=1.2)
plt.rcParams['axes.facecolor'] = '#f9f9f9'  # Light background for axes
plt.rcParams['figure.facecolor'] = '#eaeaea'  # Subtle background for the figure
plt.rcParams['grid.color'] = '#d3d3d3'  # Softer gridline colors
plt.rcParams['axes.edgecolor'] = '#a1a1a1'  # Slightly darker edges for contrast

# Create subplots
num_plots = len(categorical_cols) + len(numerical_cols)
fig, axes = plt.subplots(
    nrows=num_plots,
    ncols=1,
    figsize=(12, num_plots * 4),  # Adjusted figsize calculation
    constrained_layout=True
)

# Plot categorical features with percentages
for idx, col in enumerate(categorical_cols):
    ax = axes[idx]
    sns.countplot(
        data=data,
        x=col,
        hue='Diabetes Mellitus (DM) Status',
        palette="coolwarm",  # Stylish blue-red color palette
        ax=ax,
        dodge=True,
        width=0.6  # Slimmer bars
    )

    # Add percentage annotations
    total = len(data)
    for p in ax.patches:
        height = p.get_height()
        if height > 0:  # Avoid division by zero
            percentage = (height / total) * 100
            ax.annotate(
                f'{percentage:.0f}%',  # Rounded percentage
                (p.get_x() + p.get_width() / 2., height),
                ha='center', va='bottom', fontsize=9, color='#333333'
            )

    # Customize the plot
    ax.set_title(f'{col} vs Ischemic Heart Disease (IHD)', fontsize=14, weight='bold')
    ax.set_xlabel(col, fontsize=12)
    ax.set_ylabel('Count', fontsize=12)
    ax.legend(title='Diabetes Mellitus (DM) Status', fontsize=10)
    ax.tick_params(axis='x', rotation=30)

# Plot numerical features as boxplots
for idx, col in enumerate(numerical_cols, start=len(categorical_cols)):
    ax = axes[idx]
    sns.boxplot(data=data, x='Diabetes Mellitus (DM) Status', y=col, palette="coolwarm", ax=ax, width=0.5)  # Slimmer boxes

    # Customize the plot
    ax.set_title(f'{col} vs Ischemic Heart Disease (IHD)', fontsize=14, weight='bold')
    ax.set_xlabel('Diabetes Mellitus (DM) Status', fontsize=12)
    ax.set_ylabel(col, fontsize=12)
    ax.grid(axis='y', linestyle='--', alpha=0.6)

# Final adjustments
fig.suptitle('Diabetes Mellitus (DM) Status Analysis', fontsize=16, weight='bold')
plt.show()

# Remove rows where the 'Diabetes Mellitus (DM) Status' is 'Unknown'
data = data[data['Diabetes Mellitus (DM) Status'] != 'Unknown']

"""## Inferential Statistics

### Relation between Diabetes Mellitus (DM) Status & Ischemic Heart Disease (IHD) Status
"""

# Set a stylish background with a light gradient and muted gridlines
sns.set_theme(style="whitegrid", palette="muted", font_scale=1.2)
plt.rcParams['axes.facecolor'] = '#f9f9f9'  # Light gradient background for the axes
plt.rcParams['figure.facecolor'] = '#eaeaea'  # Subtle background for the overall figure
plt.rcParams['grid.color'] = '#d3d3d3'  # Softer gridline colors
plt.rcParams['axes.edgecolor'] = '#a1a1a1'  # Slightly darker edges for contrast

# Cross-tabulation between DM status and IHD status
DM_IHD_risk = pd.crosstab(data['Diabetes Mellitus (DM) Status'], data['Ischemic Heart Disease (IHD) Status'], margins=True, margins_name="Total")

# Drop the Total row for plotting
DM_IHD_risk.drop('Total', axis=1, inplace=True)

# Plotting a clustered bar chart for the relationship
ax = DM_IHD_risk.plot(kind='bar', figsize=(6, 4), color=['skyblue', 'lightcoral'], width=0.8)

# Title and labels
plt.title('Clustered Bar Chart: Diabetes Mellitus (DM) Status vs Ischemic Heart Disease (IHD) Status')
plt.ylabel('Count')
plt.xlabel('Diabetes Mellitus (DM) Status')
plt.xticks(rotation=0)
plt.legend(title='IHD Status', loc='upper left', labels=['No IHD', 'IHD'])

# Adding percentage annotations on top of each bar
for p in ax.patches:
    height = p.get_height()
    percentage = (height / DM_IHD_risk.sum().sum()) * 100  # Calculate percentage of total
    ax.annotate(f'{height:.0f}\n({percentage:.1f}%)',
                (p.get_x() + p.get_width() / 2., height),
                ha='center', va='center', fontsize=10,
                color='black', xytext=(0, 5), textcoords='offset points')

# Show the plot
plt.show()

"""#### Hypothesis Test Interpretation

#### Fisher’s Exact Test
###### Fisher’s Exact Test is particularly useful when dealing with small sample sizes or sparse data. It tests the null hypothesis of independence between two categorical variables.
"""

# Create the contingency table again
DM_IHD_contingency = pd.crosstab(data['Diabetes Mellitus (DM) Status'], data['Ischemic Heart Disease (IHD) Status'])

# Perform Fisher's Exact Test
oddsratio, p_value = fisher_exact(DM_IHD_contingency)

# Output the results
print(f"Odds Ratio: {oddsratio}")
print(f"P-value: {p_value}")

# Interpretation
alpha = 0.05  # Significance level
if p_value < alpha:
    print("\nThere is a significant relationship between Diabetes Mellitus (DM) Status and Ischemic Heart Disease (IHD) Status.")
else:
    print("\nThere is no significant relationship between Diabetes Mellitus (DM) Status and Ischemic Heart Disease (IHD) Status.")

"""#### McNemar's Test
###### McNemar's Test is useful when you have paired nominal data (for example, pre/post intervention data or paired observations). This method is typically applied to 2x2 contingency tables where data points are dependent.
"""

# Step 1: Create the contingency table (cross-tabulation)
DM_IHD_contingency = pd.crosstab(data['Diabetes Mellitus (DM) Status'], data['Ischemic Heart Disease (IHD) Status'])

# Step 2: Perform McNemar's Test
result = mcnemar(DM_IHD_contingency, exact=True)

# Step 3: Output the results
print(f"McNemar's Test Statistic: {result.statistic}")
print(f"P-value from McNemar's Test: {result.pvalue}")

# Step 4: Hypothesis Test Interpretation
alpha = 0.05  # Significance level
if result.pvalue < alpha:
    print("\nThere is a significant relationship between Diabetes Mellitus (DM) Status and Ischemic Heart Disease (IHD) Status.")
else:
    print("\nThere is no significant relationship between Diabetes Mellitus (DM) Status and Ischemic Heart Disease (IHD) Status.")

"""#### The Chi-Square Test of Independence"""

import pandas as pd
import scipy.stats as stats

# Step 1: Create the contingency table (cross-tabulation) between DM status and IHD status
DM_IHD_contingency = pd.crosstab(data['Diabetes Mellitus (DM) Status'], data['Ischemic Heart Disease (IHD) Status'])

# Step 2: Perform the Chi-Square Test of Independence
chi2_stat, p_value, dof, expected = stats.chi2_contingency(DM_IHD_contingency)

# Step 3: Output the results
print(f"Chi-Square Statistic: {chi2_stat}")
print(f"Degrees of Freedom: {dof}")
print(f"Expected Frequencies:\n{expected}")
print(f"P-value: {p_value}")

# Step 4: Hypothesis Test Interpretation
alpha = 0.05  # Significance level

if p_value < alpha:
    print("\nThere is a significant relationship between Diabetes Mellitus (DM) Status and Ischemic Heart Disease (IHD) Status.")
else:
    print("\nThere is no significant relationship between Diabetes Mellitus (DM) Status and Ischemic Heart Disease (IHD) Status")

"""## One-hot encoding for the categorical features

"""

# Define the categorical features
Categorical_Features = ['Sex (Male/Female)', 'Occupation Type', 'Education Level',
       'Economic Status', 'Smoking Status', 'Hypertension (HTN) Status',
       'Diabetes Mellitus (DM) Status', 'Dyslipidemia Status', 'Stroke Status',
       'Ischemic Heart Disease (IHD) Status', 'Age Group',
       'Body Mass Index (BMI) Group', 'Hypertension Stage']

# Perform one-hot encoding for the categorical features
data = pd.get_dummies(data, columns=Categorical_Features, drop_first=True)

# Display the resulting DataFrame
data.info()

"""## Filter the data to include only rows where 'Diabetes Mellitus (DM) Status_Yes'"""

# Filter the data to include only rows where 'Diabetes Mellitus (DM) Status_Yes' is True (1)
df= data[data['Diabetes Mellitus (DM) Status_Yes'] == 1]

# Display the first few rows of the new dataset
df.head().T

df.shape

"""#### Correlation matrix"""

# Select only numerical columns
numerical_data = df.select_dtypes(include=[np.number])

# Compute correlation matrix
correlation_matrix = numerical_data.corr()

# Plot the heatmap for visualization
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True)
plt.title("Correlation Matrix Heatmap")
plt.show()

# Set a correlation threshold
correlation_threshold = -.5

# Identify highly correlated features
upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))

# Find index of feature columns with correlation greater than the threshold
highly_correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)]

# Print the names of features being dropped
print(f"Features being dropped (correlation > {correlation_threshold}): {highly_correlated_features}")

# Drop highly correlated features from the original dataset
filtered_datacor = df.drop(columns=highly_correlated_features)

# Output the filtered DataFrame
filtered_datacor.info()

"""## Feature Selection

#### Split it into features (X) and target (y)
"""

X = df.drop('Ischemic Heart Disease (IHD) Status_Yes', axis=1)  # Replace 'target' with your target column name
y = df['Ischemic Heart Disease (IHD) Status_Yes']

"""### Mutual Information"""

# Encode categorical features in X
X_encoded = X.apply(lambda col: LabelEncoder().fit_transform(col) if col.dtypes == 'category' else col)

# Encode the target variable y if it's categorical
y_encoded = LabelEncoder().fit_transform(y)

# Calculate Mutual Information
mi = mutual_info_classif(X_encoded, y_encoded)

# Create a DataFrame to view the Mutual Information of each feature
mi_df = pd.DataFrame({'Feature': X.columns, 'Mutual Information': mi})
mi_df = mi_df.sort_values(by='Mutual Information', ascending=False)  # Rank features by MI

# Assuming mi_df is a DataFrame with features and their Mutual Information (MI) scores
mi_df = mi_df.sort_values(by="Mutual Information", ascending=False).reset_index(drop=True)

# Add a ranking column
mi_df["Rank"] = mi_df.index + 1

# Display the top features with rankings
print("Top Features ranked by Mutual Information:\n")
print(mi_df.head(25)[["Rank", "Feature", "Mutual Information"]])

"""##### Filter Features Based on MI Threshold"""

import pandas as pd
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import LabelEncoder

# Define the threshold value for Mutual Information (e.g., 0.1)
mi_threshold = 0.01  # Assigning a value instead of using an invalid comparison

# Encode categorical features in X
X_encoded = X.apply(lambda col: LabelEncoder().fit_transform(col) if col.dtypes == 'category' else col)

# Encode the target variable y if it's categorical
y_encoded = LabelEncoder().fit_transform(y)

# Calculate Mutual Information
mi = mutual_info_classif(X_encoded, y_encoded)

# Create a DataFrame to view the Mutual Information of each feature
mi_df = pd.DataFrame({'Feature': X.columns, 'Mutual Information': mi})

# Filter features based on the MI threshold
filtered_features = mi_df[mi_df['Mutual Information'] >= mi_threshold]

# Sort by MI to see the most important features
filtered_features = filtered_features.sort_values(by='Mutual Information', ascending=False).reset_index(drop=True)

# Add a ranking column
filtered_features["Rank"] = filtered_features.index + 1

# Print the filtered features with rankings
print(f"Filtered Features (MI >= {mi_threshold}):\n")
print(filtered_features[["Rank", "Feature", "Mutual Information"]])

"""##### Visualize the selected features with a bar chart"""

# Visualize the selected features with a bar chart
plt.figure(figsize=(10, 6))
colors = sns.color_palette("cool", len(filtered_features))  # Use a gradient color palette

bar_plot = sns.barplot(
    x="Mutual Information",
    y="Feature",
    data=filtered_features,
    palette=colors
)

# Add percentages on top of the bars
for index, row in filtered_features.iterrows():
    bar_plot.text(
        row['Mutual Information'] + 0.001,  # Slightly offset to the right of the bar
        index,  # y-coordinate of the bar
        f"{row['Mutual Information']:.3f}",  # Display MI value with 3 decimal places
        color='black',
        ha="left",
        va="center",
        fontsize=10
    )

# Add labels and title
plt.title("Feature Importance Bar Chart (Mutual Information)", fontsize=10)
plt.xlabel("Mutual Information Score", fontsize=2)
plt.ylabel("Features", fontsize=12)

# Adjust layout for better visibility
plt.tight_layout()
plt.show()

"""##### Train the Final Model"""

# Step 1: Preprocess Data
# Convert categorical features to 'category' dtype
X = X.copy()
for col in X.select_dtypes(include=['object']).columns:
    X[col] = X[col].astype('category')

# Encode categorical features using OrdinalEncoder
encoder = OrdinalEncoder()
X_encoded = X.copy()
categorical_cols = X.select_dtypes(include=['category']).columns
X_encoded[categorical_cols] = encoder.fit_transform(X[categorical_cols])

# Encode target variable
y_encoded = LabelEncoder().fit_transform(y) if y.dtype == 'object' else y

# Step 2: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# Step 3: Scale Numerical Features
scaler = StandardScaler()
X_train.iloc[:, :] = scaler.fit_transform(X_train)
X_test.iloc[:, :] = scaler.transform(X_test)

# Step 4: Train Model (Random Forest Classifier)
model = RandomForestClassifier(n_estimators=200, random_state=42)
model.fit(X_train, y_train)

# Step 5: Evaluate Model
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]  # For AUC-ROC

accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)
classification_rep = classification_report(y_test, y_pred)

# Print results
print(f"Accuracy: {accuracy:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print("Classification Report:\n", classification_rep)

"""### Random Forest"""

# Initialize and train a Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Get feature importance from the trained Random Forest model
feature_importances = rf.feature_importances_

# Create a DataFrame to view the Feature Importance
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)  # Rank features by importance

# Assuming `importance_df` is a DataFrame with features and their importance scores
importance_df = importance_df.sort_values(by="Importance", ascending=False).reset_index(drop=True)

# Add a ranking column
importance_df["Rank"] = importance_df.index + 1

# Print the top  features with rankings
print("Random Forest Feature Importance (Top Features):\n")
print(importance_df.head()[["Rank", "Feature", "Importance"]])

"""##### Filter Features Based on Importance Threshold"""

# Set a threshold for feature importance
importance_threshold = 0.03  # Set your desired threshold value

# Filter features based on importance threshold
selected_features = importance_df[importance_df['Importance'] > importance_threshold]

# Assuming `selected_features` is a DataFrame with features and their importance scores
selected_features = selected_features.sort_values(by="Importance", ascending=False).reset_index(drop=True)

# Add a ranking column
selected_features["Rank"] = selected_features.index + 1

# Print the selected features with rankings
print("Selected Features Based on Importance Threshold:\n")
print(selected_features[["Rank", "Feature", "Importance"]])

"""##### Visualize the selected features with a bar chart"""

# Visualize the selected features with a bar chart
plt.figure(figsize=(10, 6))
colors = sns.color_palette("coolwarm", len(selected_features))  # Use a gradient color palette

bar_plot = sns.barplot(
    x="Importance",
    y="Feature",
    data=selected_features,
    palette=colors
)

# Add percentages on top of the bars
for index, row in selected_features.iterrows():
    bar_plot.text(
        row['Importance'] + 0.001,  # Slightly offset to the right of the bar
        index,  # y-coordinate of the bar
        f"{row['Importance'] * 100:.1f}%",  # Convert to percentage
        color='black',
        ha="left",
        va="center",
        fontsize=8
    )

# Add labels and title
plt.title("Feature Importance Bar Chart with Percentages", fontsize=10)
plt.xlabel("Importance Score", fontsize=12)
plt.ylabel("Features", fontsize=12)

# Adjust layout for better visibility
plt.tight_layout()
plt.show()

"""##### Train the Final Model"""

# One-Hot Encode categorical variables
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)  # ✅ Use sparse_output
X_encoded = pd.DataFrame(encoder.fit_transform(X.select_dtypes(include=['object'])))

# Ensure column names are preserved
X_encoded.columns = encoder.get_feature_names_out()


# Concatenate encoded categorical features with numerical features
X_final = pd.concat([X_encoded, X.select_dtypes(exclude=['object']).reset_index(drop=True)], axis=1)

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)

# Train Random Forest model
rf_final = RandomForestClassifier(n_estimators=100, random_state=42)
rf_final.fit(X_train, y_train)

# Ensure test features have the same column names as train features
X_test = X_test.reindex(columns=X_train.columns, fill_value=0)

# Predict on test set
y_pred = rf_final.predict(X_test)
y_pred_proba = rf_final.predict_proba(X_test)[:, 1]  # Use rf_final instead of model

# Evaluate model
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)  # Ensure roc_auc_score is imported

print(f'Accuracy: {accuracy:.4f}')
print(f"ROC AUC Score: {roc_auc:.4f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""#### Create new dataset by List of important features based on Random Forest"""

# List of important features based on mutual information
important_features = [
    "Age (Years)",
    "Stroke Status_Yes",
    "Weight (kg)",
    "Random Blood Sugar (mg/dL)",
    "Height (cm)",
    "Systolic Blood Pressure (mmHg)",
    "Diabetes Mellitus (DM) Status_Yes",
    "Hypertension (HTN) Status_Yes",
    "Dyslipidemia Status_Yes",
    "Diastolic Blood Pressure (mmHg)",
    'Ischemic Heart Disease (IHD) Status_Yes'
]

# Filter the original dataset to retain only the important features
data_for_modeling = df[important_features]

# Print the new dataset's shape and preview
print("Shape of the new dataset for modeling:", data_for_modeling.shape)
print("Preview of the new dataset:")
print(data_for_modeling.head())

# Save the new dataset for future use (optional)
data_for_modeling.to_csv("data_for_modeling.csv", index=False)

"""### Define New Dataset"""

df=data_for_modeling

df.info()

"""## Test Train Split

##### Dara scaling for continuous variables
"""

# Assuming your data is in a pandas DataFrame 'data'
scaler = MinMaxScaler()

# Defining the set of continuous variables
continuous_variables = {
    'Age (Years)',
    'Weight (kg)',
    'Height (cm)',
    'Systolic Blood Pressure (mmHg)',
    'Diastolic Blood Pressure (mmHg)',
    'Random Blood Sugar (mg/dL)'
}

# Assuming your DataFrame is 'data'
numerical_columns = list(continuous_variables)

# Check if all columns are present in the DataFrame
missing_columns = [col for col in numerical_columns if col not in data.columns]
if missing_columns:
    print(f"Missing columns in the DataFrame: {missing_columns}")

# Apply the scaler to the selected numerical columns
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])
data.head(5).T

# Assuming 'datacor' is your dataset and 'Heart Failure (HF)' is the target variable
X = df.drop(columns=['Ischemic Heart Disease (IHD) Status_Yes'])  # Features
y = df['Ischemic Heart Disease (IHD) Status_Yes']                # Target variable

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Print the shapes of the resulting datasets
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

"""## Beseline Funcations"""

# Generating synthetic dataset for demonstration
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Support Vector Machine': SVC(probability=True),
    'Ridge Classifier': RidgeClassifier(),
    'Quadratic Discriminant Analysis': QuadraticDiscriminantAnalysis(),
    'Linear Discriminant Analysis': LinearDiscriminantAnalysis(),
    'AdaBoost': AdaBoostClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'Extra Trees Classifier': ExtraTreesClassifier(),
    'LightGBM': lgb.LGBMClassifier(verbose=-1)
}

# Evaluate models
def evaluate_models(models, X_train, y_train, X_test, y_test):
    results = {}
    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        results[name] = accuracy
        print(f'{name}: Accuracy = {accuracy:.4f}')

    sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
    top_3_models = sorted_results[:3]
    print("\nTop 3 Models:")
    for model, acc in top_3_models:
        print(f'{model}: Accuracy = {acc:.4f}')

    best_model = top_3_models[0]
    print(f'\nBest Model: {best_model[0]} with Accuracy = {best_model[1]:.4f}')
    return results

# Run evaluation
results = evaluate_models(models, X_train, y_train, X_test, y_test)

"""# Model building

##### Original class distribution
"""

# Step 1: Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""### Apply SMOTE for class imbalance handling"""

# Step 2: Print original class distribution
original_counts = Counter(y_train)
print("Original class distribution:", original_counts)

# Step 3: Apply SMOTE for class imbalance handling
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
smote_counts = Counter(y_train_smote)
print("Class distribution after SMOTE:", smote_counts)

# Step 4: Visualize class distribution before and after SMOTE
classes = list(original_counts.keys())
original_values = [original_counts[c] for c in classes]
smote_values = [smote_counts[c] for c in classes]

plt.figure(figsize=(8, 5))
plt.plot(classes, original_values, marker='o', linestyle='-', color='blue', label="Original")
plt.plot(classes, smote_values, marker='s', linestyle='--', color='red', label="After SMOTE")
plt.xlabel("Class Labels")
plt.ylabel("Count")
plt.title("Class Distribution Before and After SMOTE")
plt.legend()
plt.grid(True)

# Step 4: Visualize class distribution before and after SMOTE
classes = list(original_counts.keys())
original_values = [original_counts[c] for c in classes]
smote_values = [smote_counts[c] for c in classes]

# Create a bar chart
plt.figure(figsize=(8, 5))

# Bar chart for original class distribution
bar_width = 0.35  # Width of the bars
index = np.arange(len(classes))  # Set the x position for the bars

plt.bar(index, original_values, bar_width, color='blue', label="Original")
plt.bar(index + bar_width, smote_values, bar_width, color='red', label="After SMOTE")

plt.xlabel("Class Labels")
plt.ylabel("Count")
plt.title("Class Distribution Before and After SMOTE")

# Add x-axis labels with class names
plt.xticks(index + bar_width / 2, classes)

# Display the legend
plt.legend()

# Add gridlines
plt.grid(True)

# Show the plot
plt.tight_layout()
plt.show()

"""## Gradient Boosting model

##### Initialize the Gradient Boosting model
"""

# Initialize the Gradient Boosting model
gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)

"""#### Define the hyperparameter grid for GridSearchCV"""

# Define parameter grid for Gradient Boosting
gb_params = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'min_samples_split': [2, 5, 10]
}

gb = GradientBoostingClassifier()
gb_grid_search = GridSearchCV(gb, param_grid=gb_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)
gb_grid_search.fit(X_train, y_train)
print(f"Best parameters for Gradient Boosting: {gb_grid_search.best_params_}")
print(f"Best accuracy for Gradient Boosting: {gb_grid_search.best_score_}")

"""##### Initialize the model with the best parameters"""

# Initialize the Gradient Boosting model with the parameters directly
gb_clf = GradientBoostingClassifier(
    learning_rate=0.01,
    max_depth=5,
    min_samples_split=10,
    n_estimators=100,
    subsample=0.8
)

# Fit the model with the training data (after SMOTE)
gb_clf.fit(X_train_smote, y_train_smote)

# Step 10: Predict on the test data
y_pred = gb_clf.predict(X_test)
y_prob = gb_clf.predict_proba(X_test)[:, 1]

"""##### Model Evalitions"""

# Step 11: Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

# Step 12: Compute loss function
test_loss = log_loss(y_test, y_prob)
print(f"Test Log Loss: {test_loss:.4f}")

# Step 13: Print classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""##### Perform cross-validation"""

# Step 14: Perform cross-validation
cv_scores = cross_val_score(gb_clf, X_train_smote, y_train_smote,
                            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
                            scoring='accuracy')

print(f"Cross-validation accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})")

"""##### confusion matrix"""

# Step 15: Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap="coolwarm",
            xticklabels=['Predicted Negatives', 'Predicted Positives'],
            yticklabels=['True Negatives', 'True Positives'])
plt.title("Confusion Matrix Heatmap for Gradient Boosting")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()

"""###### ROC Curve and AUC"""

# Step 16: ROC Curve and AUC
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Gradient Boosting')
plt.legend(loc="lower right")
plt.show()

"""#### Accuracy comparison plot"""

import matplotlib.pyplot as plt
import numpy as np

# Example data (replace with your actual accuracy values)
accuracy = 0.85  # Test accuracy
cv_scores = [0.82, 0.83, 0.84, 0.81, 0.86]  # Cross-validation accuracy scores

# Calculate mean and standard deviation for cross-validation scores
accuracy_values = [accuracy, np.mean(cv_scores)]
accuracy_std = [0, np.std(cv_scores)]

# Plotting
plt.figure(figsize=(5, 3))
bars = plt.barh(['Test Accuracy', 'Cross-validation Accuracy'], accuracy_values, xerr=accuracy_std, color=['lightgreen', 'skyblue'], capsize=10)

# Add percentages on the bars
for bar in bars:
    percentage = bar.get_width() * 100  # Convert to percentage
    plt.text(bar.get_width() / 2, bar.get_y() + bar.get_height() / 2, f"{percentage:.2f}%",
             ha='center', va='center', fontsize=12, fontweight='bold', color='black')

# Set labels and title
plt.xlabel('Accuracy')
plt.title('Comparison of Test vs Cross-validation Accuracy for Gradient Boosting')

# Show plot
plt.tight_layout()
plt.show()

# Print the results
print(f"Test Accuracy: {accuracy * 100:.2f}%")
print(f"Cross-validation Accuracy: {np.mean(cv_scores) * 100:.2f}%")
print(f"Cross-validation Accuracy Standard Deviation: {np.std(cv_scores) * 100:.2f}%")

"""##### Loss function visualization"""

# Step 18: Loss function visualization
loss_values = [test_loss, np.mean(cv_scores)]

# Define labels for the evaluation methods
labels = ['Test Loss', 'Cross-validation Loss']

# Plot the Loss values
plt.figure(figsize=(6, 4))
plt.plot(labels, loss_values, marker='o', linestyle='-', color='purple', label='Log Loss')
plt.xlabel('Evaluation Method')
plt.ylabel('Loss')
plt.title('Log Loss Before and After Cross-validation for Gradient Boosting')
plt.legend()
plt.grid(True)

# Show the plot
plt.show()

"""## Light GBM

##### Initialize the  model
"""

# Initialize LightGBM model
lgbm_model = LGBMClassifier(n_estimators=100, max_depth=-1, random_state=42)

"""##### Define the hyperparameter grid for GridSearchCV"""

# Step 6: Define the hyperparameter grid for GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'num_leaves': [31, 50, 100],
    'max_depth': [-1, 10, 20],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Step 7: Perform GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(estimator=lgbm_model, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=1, scoring='accuracy')

grid_search.fit(X_train_smote, y_train_smote)

# Step 8: Print the best parameters
print(f"Best hyperparameters: {grid_search.best_params_}")

"""##### Initialize the model with the best parameters"""

from lightgbm import LGBMClassifier

# Step 9: Initialize LightGBM with the best parameters
lgbm_clf = LGBMClassifier(
    n_estimators=200,
    learning_rate=0.01,
    num_leaves=31,
    max_depth=-1,
    subsample=0.8,
    colsample_bytree=1.0,
    random_state=42,
    verbose=-1  # Suppress verbose output
)

lgbm_clf.fit(X_train_smote, y_train_smote)

# Step 10: Predict on the test data
y_pred = lgbm_clf.predict(X_test)
y_prob = lgbm_clf.predict_proba(X_test)[:, 1]

"""##### Model Evaluations"""

# Step 11: Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

# Step 12: Compute loss function
test_loss = log_loss(y_test, y_prob)
print(f"Test Log Loss: {test_loss:.4f}")

# Step 13: Print classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""##### Perform cross-validation"""

# Step 14: Perform cross-validation
cv_scores = cross_val_score(lgbm_clf, X_train_smote, y_train_smote,
                            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
                            scoring='accuracy')

print(f"Cross-validation accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})")

"""#### confusion matrix"""

# Step 15: Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Create the heatmap with annotations showing TP, FP, FN, TN
plt.figure(figsize=(6, 5))

# Heatmap for confusion matrix
sns.heatmap(cm, annot=True, fmt='d', cmap="coolwarm",
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'],
            cbar=False, annot_kws={'size': 16})

# Add labels and title to make the plot more readable
plt.title("Confusion Matrix for Light GBM", fontsize=18)
plt.xlabel("Predicted Labels", fontsize=14)
plt.ylabel("True Labels", fontsize=14)

# Add a table for the confusion matrix result
cell_text = [
    ['True Negative (TN)', cm[0, 0]],
    ['False Positive (FP)', cm[0, 1]],
    ['False Negative (FN)', cm[1, 0]],
    ['True Positive (TP)', cm[1, 1]]
]

## Create the table
#plt.table(cellText=cell_text, colLabels=["", "Count"], loc="bottom", cellLoc="center", colColours=["#f7f7f7"]*2, bbox=[0, -0.3, 1, 0.2])

# Display the matrix with the annotations and table
#plt.show()

"""##### ROC Curve and AUC"""

# Step 16: ROC Curve and AUC
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Light GBM')
plt.legend(loc="lower right")
plt.show()

"""#####  Accuracy comparison plot"""

# Step 17: Accuracy comparison plot
labels = ['Test Accuracy', 'Cross-validation Accuracy']
accuracy_values = [accuracy, np.mean(cv_scores)]
accuracy_std = [0, np.std(cv_scores)]
plt.figure(figsize=(5, 3))
bars = plt.barh(labels, accuracy_values, xerr=accuracy_std, color=['lightgreen', 'skyblue'], capsize=10)
for bar in bars:
    plt.text(bar.get_width()/2, bar.get_y() + bar.get_height()/2, f"{bar.get_width():.4f}",
             ha='center', va='center', fontsize=12, fontweight='bold', color='black')
plt.xlabel('Accuracy')
plt.title('Comparison of Test vs Cross-validation Accuracy for Light GBM')
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Accuracy values (assuming 'accuracy' and 'cv_scores' are already defined)
labels = ['Test Accuracy', 'Cross-validation Accuracy']
accuracy_values = [accuracy, np.mean(cv_scores)]  # accuracy should be a float (0 to 1)
accuracy_std = [0, np.std(cv_scores)]  # standard deviation for cross-validation accuracy

# Plotting the horizontal bar chart
plt.figure(figsize=(5, 3))
bars = plt.barh(labels, accuracy_values, xerr=accuracy_std, color=['lightgreen', 'skyblue'], capsize=10)

# Annotating with percentages
for bar in bars:
    percentage = bar.get_width() * 100  # Convert to percentage
    plt.text(bar.get_width()/2, bar.get_y() + bar.get_height()/2, f"{percentage:.2f}%",
             ha='center', va='center', fontsize=12, fontweight='bold', color='black')

# Labels and title
plt.xlabel('Accuracy (%)')
plt.title('Comparison of Test vs Cross-validation Accuracy for Light GBM')

# Show the plot
plt.tight_layout()
plt.show()

 #Print the results as percentage values
print(f"Test Accuracy: {accuracy * 100:.2f}%")
print(f"Cross-validation Accuracy: {np.mean(cv_scores) * 100:.2f}%")
print(f"Cross-validation Accuracy Std Dev: {np.std(cv_scores) * 100:.2f}%")

"""##### Loss function visualization"""

# Step 18: Loss function visualization
loss_values = [test_loss, np.mean(cv_scores)]
plt.figure(figsize=(8, 5))
plt.plot(labels, loss_values, marker='o', linestyle='-', color='purple', label='Log Loss')
plt.xlabel('Evaluation Method')
plt.ylabel('Loss')
plt.title('Log Loss Before and After Cross-validation for Light GBM')
plt.legend()
plt.grid(True)
plt.show()

"""## Random Forest

##### Initialize the model
"""

# Initialize the Random Forest model
rf_model = RandomForestClassifier()

"""##### Define the hyperparameter grid to search over"""

# Define the hyperparameter grid to search over
param_grid = {
    'n_estimators': [50, 100, 200, 300],  # Number of trees in the forest
    'max_depth': [None, 10, 20, 30],       # Maximum depth of the trees
    'min_samples_split': [2, 5, 10],       # Minimum number of samples required to split a node
    'min_samples_leaf': [1, 2, 4],         # Minimum number of samples required at each leaf node
    'bootstrap': [True, False]             # Whether bootstrap samples are used when building trees
}

 # Step 7: Perform GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=1, scoring='accuracy')

grid_search.fit(X_train_smote, y_train_smote)

# Step 8: Print the best parameters
print(f"Best hyperparameters: {grid_search.best_params_}")

"""##### Initialize the Random Forest model with the best hyperparameters"""

# Initialize the Random Forest model with the best hyperparameters
rf_clf = RandomForestClassifier(bootstrap=False, max_depth=None, min_samples_leaf=1,
                                min_samples_split=5, n_estimators=50, random_state=42)

# Step 10: Fit the model on the training data
rf_clf.fit(X_train_smote, y_train_smote)

# Predict on the test data
y_pred = rf_clf.predict(X_test)
y_prob = rf_clf.predict_proba(X_test)[:, 1]

"""##### Model Evaluations"""

# Step 11: Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")

# Step 12: Compute loss function
test_loss = log_loss(y_test, y_prob)
print(f"Test Log Loss: {test_loss:.4f}")

# Step 13: Print classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

"""##### Perform cross-validation"""

# Step 14: Perform cross-validation
cv_scores = cross_val_score(rf_model, X_train_smote, y_train_smote,
                            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
                            scoring='accuracy')

print(f"Cross-validation accuracy: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})")

"""##### Generate confusion matrix"""

# Step 15: Generate confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Create the heatmap with annotations showing TP, FP, FN, TN
plt.figure(figsize=(6, 5))

# Heatmap for confusion matrix
sns.heatmap(cm, annot=True, fmt='d', cmap="coolwarm",
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'],
            cbar=False, annot_kws={'size': 16})

# Add labels and title to make the plot more readable
plt.title("Confusion Matrix for Random Forest", fontsize=18)
plt.xlabel("Predicted Labels", fontsize=14)
plt.ylabel("True Labels", fontsize=14)

# Add a table for the confusion matrix result
cell_text = [
    ['True Negative (TN)', cm[0, 0]],
    ['False Positive (FP)', cm[0, 1]],
    ['False Negative (FN)', cm[1, 0]],
    ['True Positive (TP)', cm[1, 1]]
]

# Create the table
#plt.table(cellText=cell_text, colLabels=["", "Count"], loc="bottom", cellLoc="center", colColours=["#f7f7f7"]*2, bbox=[0, -0.3, 1, 0.2])

# Display the matrix with the annotations and table
#plt.show()

"""##### ROC Curve and AUC"""

# Step 16: ROC Curve and AUC
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Random Forest')
plt.legend(loc="lower right")
plt.show()

"""##### Accuracy comparison plot"""

import numpy as np
import matplotlib.pyplot as plt

# Example accuracy values (replace with actual results from your model evaluation)
accuracy = 0.85  # Example test accuracy
cv_scores = [0.84, 0.86, 0.83, 0.87, 0.85]  # Example cross-validation scores

# Calculate standard deviation for cross-validation scores
accuracy_values = [accuracy, np.mean(cv_scores)]
accuracy_std = [0, np.std(cv_scores)]

# Plot comparison
plt.figure(figsize=(5, 3))
bars = plt.barh(['Test Accuracy', 'Cross-validation Accuracy'], accuracy_values, xerr=accuracy_std, color=['lightgreen', 'skyblue'], capsize=10)

# Add text annotations to display the percentage on the bars
for bar in bars:
    percentage = f"{bar.get_width() * 100:.2f}%"  # Convert to percentage and format to 2 decimal places
    plt.text(bar.get_width() / 2, bar.get_y() + bar.get_height() / 2, percentage,
             ha='center', va='center', fontsize=12, fontweight='bold', color='black')

# Set labels and title
plt.xlabel('Accuracy')
plt.title('Comparison of Test vs Cross-validation Accuracy for Random Forest')

# Show plot
plt.tight_layout()
plt.show()

# Print results
print("Accuracy Results:")
print(f"Test Accuracy: {accuracy * 100:.2f}%")
print(f"Cross-validation Accuracy: {np.mean(cv_scores) * 100:.2f}%")
print(f"Cross-validation Standard Deviation: {np.std(cv_scores):.4f}")

"""##### Loss function visualization"""

# Step 18: Loss function visualization
loss_values = [test_loss, np.mean(cv_scores)]
plt.figure(figsize=(8, 5))
plt.plot(labels, loss_values, marker='o', linestyle='-', color='purple', label='Log Loss')
plt.xlabel('Evaluation Method')
plt.ylabel('Loss')
plt.title('Log Loss Before and After Cross-validation for Random Forest')
plt.legend()
plt.grid(True)
plt.show()





"""# Model Evulation

#### Model Comparism
"""

# Define the function to evaluate models
def evaluate_model_performance(X_train, y_train, X_test, y_test):
    models = {
        'Gradient Boosting':GradientBoostingClassifier(
    learning_rate=0.01,
    max_depth=5,
    min_samples_split=10,
    n_estimators=100,
    subsample=0.8
),


        'Light GBMC': LGBMClassifier(
    n_estimators=200,
    learning_rate=0.01,
    num_leaves=31,
    max_depth=-1,
    subsample=0.8,
    colsample_bytree=1.0,
    random_state=42
),

        'Random Forest': RandomForestClassifier(bootstrap=False, max_depth=None, min_samples_leaf=1,
                                min_samples_split=5, n_estimators=50, random_state=42)}


    # Initialize a DataFrame to store the results
    results = []

    # Loop through each model, fit, predict, and evaluate performance
    for model_name, model in models.items():
        # Fit the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)

        # Generate the classification report
        class_report = classification_report(y_test, y_pred, output_dict=True)

        # Safely extract precision, recall, and f1-scores for each class
        precision_0 = class_report['0']['precision'] if '0' in class_report else None
        recall_0 = class_report['0']['recall'] if '0' in class_report else None
        f1_0 = class_report['0']['f1-score'] if '0' in class_report else None

        precision_1 = class_report['1']['precision'] if '1' in class_report else None
        recall_1 = class_report['1']['recall'] if '1' in class_report else None
        f1_1 = class_report['1']['f1-score'] if '1' in class_report else None

        # Generate the confusion matrix
        cm = confusion_matrix(y_test, y_pred)

        # Store the results in the dataframe
        results.append({
            'Model': model_name,
            'Accuracy': accuracy,
            'Precision (Class 0)': precision_0,
            'Recall (Class 0)': recall_0,
            'F1 Score (Class 0)': f1_0,
            'Precision (Class 1)': precision_1,
            'Recall (Class 1)': recall_1,
            'F1 Score (Class 1)': f1_1,
            'Confusion Matrix': cm
        })

    # Convert results to DataFrame for comparison
    results_df = pd.DataFrame(results)

    return results_df

# Example of how to use the function
# Assuming X_train, y_train, X_test, y_test are already defined:

# Call the function and store the results in a DataFrame
results_df = evaluate_model_performance(X_train, y_train, X_test, y_test)

# Now print the results DataFrame
print(results_df)

"""#### 'Model Comparison Across Different Metrics'"""

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from lightgbm import LGBMClassifier

# Define the function to evaluate models and visualize comparison
def evaluate_model_performance(X_train, y_train, X_test, y_test):
    models = {
        'Gradient Boosting': GradientBoostingClassifier(
            learning_rate=0.01,
            max_depth=5,
            min_samples_split=10,
            n_estimators=100,
            subsample=0.8
        ),

        'Light GBMC': LGBMClassifier(
            n_estimators=200,
            learning_rate=0.01,
            num_leaves=31,
            max_depth=-1,
            subsample=0.8,
            colsample_bytree=1.0,
            random_state=42
        ),

        'Random Forest': RandomForestClassifier(
            bootstrap=False,
            max_depth=None,
            min_samples_leaf=1,
            min_samples_split=5,
            n_estimators=50,
            random_state=42
        )
    }

    # Initialize a list to store the results
    results = []

    # Loop through each model, fit, predict, and evaluate performance
    for model_name, model in models.items():
        # Fit the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)

        # Generate the classification report
        class_report = classification_report(y_test, y_pred, output_dict=True)

        # Extract overall precision, recall, and f1-score (macro averages)
        precision = class_report['macro avg']['precision']
        recall = class_report['macro avg']['recall']
        f1 = class_report['macro avg']['f1-score']

        # Store the results in the list
        results.append({
            'Model': model_name,
            'Accuracy': accuracy,
            'Precision': precision,
            'Recall': recall,
            'F1 Score': f1
        })

    # Convert results to DataFrame for comparison
    results_df = pd.DataFrame(results)

    # Set colors for the bars (using a distinct color for each metric)
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # Custom colors for the metrics

    # Plot a grouped bar chart for the metrics
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']

    fig, ax = plt.subplots(figsize=(10, 6))

    # Loop through each metric and plot the bars with different colors
    width = 0.2  # Width of each bar
    x = range(len(results_df))

    for i, metric in enumerate(metrics):
        bars = ax.bar(
            [p + width * i for p in x],
            results_df[metric],
            width=width,
            label=metric,
            color=colors[i]
        )

        # Adding integer percentage labels slightly above each bar
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2, height + 0.02, f'{height * 100:.2f}%',  # Slightly above the bar
                    ha='center', va='bottom', fontsize=10)  # Show percentage with two decimal places

    # Set labels, title, and legends
    ax.set_xticks([p + width * 1.5 for p in x])  # Center the x-ticks for the grouped bars
    ax.set_xticklabels(results_df['Model'])
    ax.set_ylabel('Score')
    ax.set_title('Model Comparison Across Different Metrics')
    ax.legend()

    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    return results_df

# Example of how to use the function
# Assuming X_train, y_train, X_test, y_test are already defined:
# Call the function and store the results in a DataFrame
results_df = evaluate_model_performance(X_train, y_train, X_test, y_test)

"""##### 'Model Performance: Before and After Hyperparameter Tuning'"""

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from lightgbm import LGBMClassifier

# Define the function to evaluate models
def evaluate_model_performance(X_train, y_train, X_test, y_test):
    # Initial Models (Before Hyperparameter Tuning)
    models = {
        'Gradient Boosting': GradientBoostingClassifier(
            learning_rate=0.01,
            max_depth=5,
            min_samples_split=10,
            n_estimators=100,
            subsample=0.8
        ),

        'Light GBMC': LGBMClassifier(
            n_estimators=200,
            learning_rate=0.01,
            num_leaves=31,
            max_depth=-1,
            subsample=0.8,
            colsample_bytree=1.0,
            random_state=42
        ),

        'Random Forest': RandomForestClassifier(
            bootstrap=False,
            max_depth=None,
            min_samples_leaf=1,
            min_samples_split=5,
            n_estimators=50,
            random_state=42
        )
    }

    # Hyperparameter tuning using GridSearchCV for each model
    tuned_models = {
        'Gradient Boosting': GridSearchCV(
            GradientBoostingClassifier(),
            param_grid={
                'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'min_samples_split': [2, 5, 10]
            },
            cv=3, scoring='accuracy', n_jobs=-1
        ),

        'Light GBMC': GridSearchCV(
            LGBMClassifier(random_state=42),
            param_grid={
                'n_estimators': [100, 200, 300],
                 'learning_rate': [0.01, 0.05, 0.1],
                 'num_leaves': [31, 50, 100],
                 'max_depth': [-1, 10, 20],
                'subsample': [0.8, 1.0],
                 'colsample_bytree': [0.8, 1.0]
            },
            cv=3, scoring='accuracy', n_jobs=-1
        ),

        'Random Forest': GridSearchCV(
            RandomForestClassifier(random_state=42),
            param_grid={
                'n_estimators': [50, 100, 200, 300],  # Number of trees in the forest
                 'max_depth': [None, 10, 20, 30],       # Maximum depth of the trees
                'min_samples_split': [2, 5, 10],       # Minimum number of samples required to split a node
                 'min_samples_leaf': [1, 2, 4],         # Minimum number of samples required at each leaf node
                  'bootstrap': [True, False]             # Whether bootstrap samples are used when building trees
            },
            cv=3, scoring='accuracy', n_jobs=-1
        )
    }

    # Store results in a list
    results_before_tuning = []
    results_after_tuning = []

    # Evaluate models before tuning (using default hyperparameters)
    for model_name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)

        results_before_tuning.append({
            'Model': model_name,
            'Accuracy (Before Tuning)': accuracy
        })

    # Evaluate models after tuning (using GridSearchCV)
    for model_name, grid_search in tuned_models.items():
        grid_search.fit(X_train, y_train)
        best_model = grid_search.best_estimator_
        y_pred_tuned = best_model.predict(X_test)
        accuracy_tuned = accuracy_score(y_test, y_pred_tuned)

        results_after_tuning.append({
            'Model': model_name,
            'Accuracy (After Tuning)': accuracy_tuned
        })

    # Create DataFrames for both pre- and post-tuning results
    before_tuning_df = pd.DataFrame(results_before_tuning)
    after_tuning_df = pd.DataFrame(results_after_tuning)

    # Merge the results DataFrames
    comparison_df = pd.merge(before_tuning_df, after_tuning_df, on='Model')

    # Plot the comparison graph (Before and After Hyperparameter Tuning)
    ax = comparison_df.set_index('Model').plot(kind='bar', figsize=(12, 6), color=['salmon', 'deepskyblue'])
    plt.title('Model Performance: Before and After Hyperparameter Tuning')
    plt.ylabel('Accuracy')
    plt.xlabel('Model')
    plt.xticks(rotation=0)
    plt.tight_layout()

    # Add percentage on top of the bars
    for p in ax.patches:
        ax.annotate(f'{p.get_height()*100:.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center', fontsize=12, color='black', xytext=(0, 8),
                    textcoords='offset points')

    plt.show()

    return comparison_df

# Example of how to use the function
# Assuming X_train, y_train, X_test, y_test are already defined:
comparison_df = evaluate_model_performance(X_train, y_train, X_test, y_test)



"""#### 'Model Performance: Accuracy (Train-Test Split) vs Accuracy (Cross-Validation)'"""

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from lightgbm import LGBMClassifier

# Define the function to evaluate models
def evaluate_model_performance(X_train, y_train, X_test, y_test):
    models = {
        'Gradient Boosting': GradientBoostingClassifier(
            learning_rate=0.01,
            max_depth=5,
            min_samples_split=10,
            n_estimators=100,
            subsample=0.8
        ),

        'Light GBMC': LGBMClassifier(
            n_estimators=200,
            learning_rate=0.01,
            num_leaves=31,
            max_depth=-1,
            subsample=0.8,
            colsample_bytree=1.0,
            random_state=42
        ),

        'Random Forest': RandomForestClassifier(
            bootstrap=False,
            max_depth=None,
            min_samples_leaf=1,
            min_samples_split=5,
            n_estimators=50,
            random_state=42
        )
    }

    # Initialize a DataFrame to store the results
    results = []

    # Now, perform cross-validation and compute mean accuracy for each model
    cv_results = []
    for model_name, model in models.items():
        # Fit the model on the training data
        model.fit(X_train, y_train)

        # Make predictions on the test set
        y_pred = model.predict(X_test)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)

        # Store the results in the dataframe
        results.append({
            'Model': model_name,
            'Accuracy': accuracy
        })

        # Perform cross-validation and calculate mean accuracy
        cv_accuracy = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy').mean()
        cv_results.append({
            'Model': model_name,
            'Accuracy (Cross-validation)': cv_accuracy
        })

    # Convert results to DataFrame for comparison
    results_df = pd.DataFrame(results)
    cv_results_df = pd.DataFrame(cv_results)

    # Merge the results DataFrame with the cross-validation results
    comparison_df = pd.merge(results_df[['Model', 'Accuracy']], cv_results_df[['Model', 'Accuracy (Cross-validation)']], on='Model')

    # Plot the comparison graph
    ax = comparison_df.set_index('Model').plot(kind='bar', figsize=(10, 6),color=['lightgreen', 'skyblue'])
    plt.title('Model Performance: Accuracy (Train-Test Split) vs Accuracy (Cross-Validation)')
    plt.ylabel('Accuracy')
    plt.xlabel('Model')
    plt.xticks(rotation=0)
    plt.tight_layout()

    # Add percentage on top of the bars
    for p in ax.patches:
        ax.annotate(f'{p.get_height()*100:.2f}%', (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center', fontsize=12, color='black', xytext=(0, 8),
                    textcoords='offset points')

    plt.show()

    return comparison_df

# Example of how to use the function
# Assuming X_train, y_train, X_test, y_test are already defined:

# Call the function and store the results in a DataFrame
comparison_df = evaluate_model_performance(X_train, y_train, X_test, y_test)

# Now print the comparison DataFrame
#print(comparison_df)





"""#### ROC curves"""

# Define the function to evaluate models
def evaluate_model_performance(X_train, y_train, X_test, y_test):
    models = {
        'Gradient Boosting': GradientBoostingClassifier(
            learning_rate=0.01,
            max_depth=5,
            min_samples_split=10,
            n_estimators=100,
            subsample=0.8
        ),

        'LightGBM': LGBMClassifier(
            n_estimators=200,
            learning_rate=0.01,
            num_leaves=31,
            max_depth=-1,
            subsample=0.8,
            colsample_bytree=1.0,
            random_state=42
        ),

        'Random Forest': RandomForestClassifier(
            bootstrap=False, max_depth=None, min_samples_leaf=1,
            min_samples_split=5, n_estimators=50, random_state=42
        )
    }


    # Initialize a DataFrame to store the results
    results = []

    # Prepare a plot for ROC curve
    plt.figure(figsize=(10, 8))

    # Loop through each model, fit, predict, and evaluate performance
    for model_name, model in models.items():
        # Fit the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)

        # Generate the classification report
        class_report = classification_report(y_test, y_pred, output_dict=True)

        # Safely extract precision, recall, and f1-scores for each class
        precision_0 = class_report['0']['precision'] if '0' in class_report else None
        recall_0 = class_report['0']['recall'] if '0' in class_report else None
        f1_0 = class_report['0']['f1-score'] if '0' in class_report else None

        precision_1 = class_report['1']['precision'] if '1' in class_report else None
        recall_1 = class_report['1']['recall'] if '1' in class_report else None
        f1_1 = class_report['1']['f1-score'] if '1' in class_report else None

        # Generate the confusion matrix
        cm = confusion_matrix(y_test, y_pred)

        # Store the results in the dataframe
        results.append({
            'Model': model_name,
            'Accuracy': accuracy,
            'Precision (Class 0)': precision_0,
            'Recall (Class 0)': recall_0,
            'F1 Score (Class 0)': f1_0,
            'Precision (Class 1)': precision_1,
            'Recall (Class 1)': recall_1,
            'F1 Score (Class 1)': f1_1,
            'Confusion Matrix': cm
        })

        # Check if the model supports probability predictions
        if hasattr(model, "predict_proba"):
            y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class
            fpr, tpr, _ = roc_curve(y_test, y_prob)
            roc_auc = auc(fpr, tpr)

            # Plot ROC curve for this model
            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')

    # Plot formatting for the ROC curve
    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC): Gradient Boosting vs LightGBM vs Random Forest')
    plt.legend(loc='lower right')
    plt.show()

    # Convert results to DataFrame for comparison
    results_df = pd.DataFrame(results)

    return results_df

# Example of how to use the function
# Assuming X_train, y_train, X_test, y_test are already defined:

# Call the function and store the results in a DataFrame
results_df = evaluate_model_performance(X_train, y_train, X_test, y_test)

# Now print the results DataFrame
#print(results_df)

"""### precision, recall"""

# Define the function to evaluate models
def evaluate_model_performance(X_train, y_train, X_test, y_test):
    models = {
        'Gradient Boosting': GradientBoostingClassifier(
            learning_rate=0.01,
            max_depth=5,
            min_samples_split=10,
            n_estimators=100,
            subsample=0.8
        ),

        'LightGBM': LGBMClassifier(
            n_estimators=200,
            learning_rate=0.01,
            num_leaves=31,
            max_depth=-1,
            subsample=0.8,
            colsample_bytree=1.0,
            random_state=42
        ),

        'Random Forest': RandomForestClassifier(
            bootstrap=False, max_depth=None, min_samples_leaf=1,
            min_samples_split=5, n_estimators=50, random_state=42
        )
    }

    # Initialize a DataFrame to store the results
    results = []

    # Prepare a plot for Precision-Recall curve
    plt.figure(figsize=(10, 8))

    # Loop through each model, fit, predict, and evaluate performance
    for model_name, model in models.items():
        # Fit the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)

        # Generate the classification report
        class_report = classification_report(y_test, y_pred, output_dict=True)

        # Safely extract precision, recall, and f1-scores for each class
        precision_0 = class_report['0']['precision'] if '0' in class_report else None
        recall_0 = class_report['0']['recall'] if '0' in class_report else None
        f1_0 = class_report['0']['f1-score'] if '0' in class_report else None

        precision_1 = class_report['1']['precision'] if '1' in class_report else None
        recall_1 = class_report['1']['recall'] if '1' in class_report else None
        f1_1 = class_report['1']['f1-score'] if '1' in class_report else None

        # Generate the confusion matrix
        cm = confusion_matrix(y_test, y_pred)

        # Store the results in the dataframe
        results.append({
            'Model': model_name,
            'Accuracy': accuracy,
            'Precision (Class 0)': precision_0,
            'Recall (Class 0)': recall_0,
            'F1 Score (Class 0)': f1_0,
            'Precision (Class 1)': precision_1,
            'Recall (Class 1)': recall_1,
            'F1 Score (Class 1)': f1_1,
            'Confusion Matrix': cm
        })

        # Check if the model supports probability predictions
        if hasattr(model, "predict_proba"):
            y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class
            precision, recall, _ = precision_recall_curve(y_test, y_prob)
            pr_auc = auc(recall, precision)

            # Plot Precision-Recall curve for this model
            plt.plot(recall, precision, label=f'{model_name} (AUC = {pr_auc:.2f})')

    # Plot formatting for the Precision-Recall curve
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve for: Gradient Boosting vs LightGBM vs Random Forest')
    plt.legend(loc='lower left')
    plt.show()

    # Convert results to DataFrame for comparison
    results_df = pd.DataFrame(results)

    return results_df

# Example of how to use the function
# Assuming X_train, y_train, X_test, y_test are already defined:

# Call the function and store the results in a DataFrame
results_df = evaluate_model_performance(X_train, y_train, X_test, y_test)

# Now print the results DataFrame
##print(results_df)

"""## Model Comparison: Accuracy and ROC AUC"""



"""## logloss funcation"""

from sklearn.metrics import log_loss, accuracy_score
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from lightgbm import LGBMClassifier
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Define the function to evaluate models
def evaluate_model_performance_logloss(X_train, y_train, X_test, y_test):
    # Initialize individual classifiers with their best hyperparameters
    models = {
        'Gradient Boosting': GradientBoostingClassifier(
            learning_rate=0.01,
            max_depth=5,
            min_samples_split=10,
            n_estimators=100,
            subsample=0.8
        ),

        'LightGBM': LGBMClassifier(
            n_estimators=200,
            learning_rate=0.01,
            num_leaves=31,
            max_depth=-1,
            subsample=0.8,
            colsample_bytree=1.0,
            random_state=42
        ),

        'Random Forest': RandomForestClassifier(
            bootstrap=False, max_depth=None, min_samples_leaf=1,
            min_samples_split=5, n_estimators=50, random_state=42
        )
    }

    # Initialize lists to store results
    accuracy_scores = []
    logloss_scores = []

    # Loop through each model, fit, predict, and evaluate performance
    for model_name, model in models.items():
        # Fit the model
        model.fit(X_train, y_train)

        # Make predictions and probability predictions
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class

        # Calculate accuracy before applying Log-Loss
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)

        # Calculate Log-Loss
        logloss = log_loss(y_test, y_prob)
        logloss_scores.append(logloss)

    # Find the best (lowest) Log-Loss score
    best_logloss = min(logloss_scores)

    # Calculate Performance (%) based on Log-Loss
    logloss_percentages = [(1 - (loss - best_logloss) / best_logloss) * 100 for loss in logloss_scores]
    logloss_percentages = [round(p) for p in logloss_percentages]  # Round percentages to avoid decimals

    # Create a DataFrame for comparison
    comparison_df = pd.DataFrame({
        'Model': list(models.keys()),
        'Accuracy (Before Log-Loss)': accuracy_scores,
        'Log-Loss': logloss_scores,
        'Performance (%)': logloss_percentages
    })

    # Plotting the Comparison
    plt.figure(figsize=(12, 6))

    # Plot accuracy before Log-Loss
    plt.bar(comparison_df['Model'], [round(acc * 100) for acc in comparison_df['Accuracy (Before Log-Loss)']],
            color='green', alpha=0.6, label='Accuracy (Before Log-Loss)', width=0.4, align='center')

    # Plot Log-Loss and Performance after Log-Loss
    plt.bar(comparison_df['Model'], comparison_df['Log-Loss'], color='red', alpha=0.6, label='Log-Loss', width=0.4, align='edge')
    plt.bar(comparison_df['Model'], comparison_df['Performance (%)'], color='skyblue', alpha=0.6, label='Performance (%)', width=0.4, align='edge')

    # Annotating the bars
    for i, row in comparison_df.iterrows():
        plt.text(i, row['Accuracy (Before Log-Loss)'] * 100, f"{round(row['Accuracy (Before Log-Loss)'] * 100)}%",
                 ha='center', va='bottom', fontsize=12, fontweight='bold', color='black')
        plt.text(i, row['Log-Loss'], f"{round(row['Log-Loss'], 4)}", ha='center', va='bottom', fontsize=12, fontweight='bold', color='black')
        plt.text(i, row['Performance (%)'], f"{row['Performance (%)']}%", ha='center', va='bottom', fontsize=12, fontweight='bold', color='black')

    # Set plot labels and title
    plt.title('Model Performance: Before and After Log-Loss: Log-Loss: Gradient Boosting vs LightGBM vs Random Forest', fontsize=14)
    plt.xlabel('Model')
    plt.ylabel('Scores')
    plt.legend(loc='upper left')
    plt.tight_layout()
    plt.show()

    return comparison_df

# Example of how to call the function:
# Assuming X_train, y_train, X_test, y_test are already defined
comparison_df = evaluate_model_performance_logloss(X_train, y_train, X_test, y_test)
# Print the resulting comparison DataFrame
print(comparison_df)

"""## Soft voting ensemble method"""

from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier, RandomForestClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize individual classifiers with their best hyperparameters
models = {
    'Gradient Boosting': GradientBoostingClassifier(
        learning_rate=0.01,
        max_depth=5,
        min_samples_split=10,
        n_estimators=100,
        subsample=0.8
    ),

    'LightGBM': LGBMClassifier(
        n_estimators=200,
        learning_rate=0.01,
        num_leaves=31,
        max_depth=-1,
        subsample=0.8,
        colsample_bytree=1.0,
        random_state=42
    ),

    'Random Forest': RandomForestClassifier(
        bootstrap=False, max_depth=None, min_samples_leaf=1,
        min_samples_split=5, n_estimators=50, random_state=42
    )
}

# Convert models dictionary into a list of tuples for VotingClassifier
classifiers = list(models.items())

# Create the VotingClassifier with soft voting
voting_clf = VotingClassifier(estimators=classifiers, voting='soft')

# Fit the ensemble model
voting_clf.fit(X_train, y_train)

# Predict on the test set
y_pred = voting_clf.predict(X_test)

# Calculate accuracy for the ensemble
ensemble_accuracy = accuracy_score(y_test, y_pred)
print(f"Soft Voting Ensemble Accuracy: {ensemble_accuracy:.4f}")

# Evaluate and display accuracy for each individual classifier
print("\nIndividual Classifier Accuracies:")
best_model = None
best_accuracy = 0

for name, clf in models.items():  # Loop through the original models dictionary
    clf.fit(X_train, y_train)  # Ensure each classifier is trained
    y_pred_individual = clf.predict(X_test)
    individual_accuracy = accuracy_score(y_test, y_pred_individual)
    print(f"{name} Accuracy: {individual_accuracy:.4f}")

    # Track the best model
    if individual_accuracy > best_accuracy:
        best_accuracy = individual_accuracy
        best_model = name

# Display the best-performing model
print(f"\nBest Model: {best_model} with Accuracy: {best_accuracy:.4f}")

# Classification report for the ensemble
print("\nClassification Report for Soft Voting Ensemble:")
print(classification_report(y_test, y_pred))

"""##### Generate the confusion matrix"""

# Generate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Extract values
TN, FP, FN, TP = cm.ravel()

# Create a heatmap using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap="cool",
            xticklabels=['Predicted Negatives', 'Predicted Positives'],
            yticklabels=['True Negatives', 'True Positives'],color=['skyblue', 'lightcoral','red'])
plt.title("Confusion Matrix Heatmap for Soft Voting Ensemble")
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.show()



# Step 16: ROC Curve and AUC
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Soft Voting Ensemble')
plt.legend(loc="lower right")
plt.show()



import numpy as np
import matplotlib.pyplot as plt

# Example accuracy values (replace with actual results from your model evaluation)
accuracy = 0.85  # Example test accuracy
cv_scores = [0.84, 0.86, 0.83, 0.87, 0.85]  # Example cross-validation scores

# Calculate standard deviation for cross-validation scores
accuracy_values = [accuracy, np.mean(cv_scores)]
accuracy_std = [0, np.std(cv_scores)]

# Plot comparison
plt.figure(figsize=(5, 3))
bars = plt.barh(['Test Accuracy', 'Cross-validation Accuracy'], accuracy_values, xerr=accuracy_std, color=['lightgreen', 'skyblue'], capsize=10)

# Add text annotations to display the percentage on the bars
for bar in bars:
    percentage = f"{bar.get_width() * 100:.2f}%"  # Convert to percentage and format to 2 decimal places
    plt.text(bar.get_width() / 2, bar.get_y() + bar.get_height() / 2, percentage,
             ha='center', va='center', fontsize=12, fontweight='bold', color='black')

# Set labels and title
plt.xlabel('Accuracy')
plt.title('Comparison of Test vs Cross-validation Accuracy for Soft Voting Ensemble')

# Show plot
plt.tight_layout()
plt.show()

# Print results
#print("Accuracy Results:")
#print(f"Test Accuracy: {accuracy * 100:.2f}%")
#print(f"Cross-validation Accuracy: {np.mean(cv_scores) * 100:.2f}%")
#print(f"Cross-validation Standard Deviation: {np.std(cv_scores):.4f}")



# Step 18: Loss function visualization
loss_values = [test_loss, np.mean(cv_scores)]
plt.figure(figsize=(8, 5))
plt.plot(labels, loss_values, marker='o', linestyle='-', color='purple', label='Log Loss')
plt.xlabel('Evaluation Method')
plt.ylabel('Loss')
plt.title('Log Loss Before and After Cross-validation for Soft Voting Ensemble')
plt.legend()
plt.grid(True)
plt.show()

"""##### Performance Evaluation: Gradient Boosting Vs Random Forest Vs LightGBM Vs Soft voting ensemble"""

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score
from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier, RandomForestClassifier
from lightgbm import LGBMClassifier

# Initialize individual classifiers with their best hyperparameters
models = {
    'Gradient Boosting': GradientBoostingClassifier(
        learning_rate=0.01,
        max_depth=5,
        min_samples_split=10,
        n_estimators=100,
        subsample=0.8
    ),

    'LightGBM': LGBMClassifier(
        n_estimators=200,
        learning_rate=0.01,
        num_leaves=31,
        max_depth=-1,
        subsample=0.8,
        colsample_bytree=1.0,
        random_state=42
    ),

    'Random Forest': RandomForestClassifier(
        bootstrap=False, max_depth=None, min_samples_leaf=1,
        min_samples_split=5, n_estimators=50, random_state=42
    )
}

# Convert models dictionary into a list of tuples for VotingClassifier
classifiers = list(models.items())

# Create the VotingClassifier with soft voting
voting_clf = VotingClassifier(estimators=classifiers, voting='soft')

# Fit the ensemble model
voting_clf.fit(X_train, y_train)

# Predict on the test set
y_pred = voting_clf.predict(X_test)

# Calculate accuracy for the ensemble
ensemble_accuracy = accuracy_score(y_test, y_pred)

# Initialize the metrics dictionary
metrics = {
    'Model': ['Soft Voting Ensemble'],
    'Accuracy': [ensemble_accuracy],
    'Precision': [],
    'Recall': [],
    'F1 Score': []
}

# Calculate metrics for the soft voting ensemble
ensemble_precision = precision_score(y_test, y_pred)
ensemble_recall = recall_score(y_test, y_pred)
ensemble_f1 = f1_score(y_test, y_pred)

metrics['Precision'].append(ensemble_precision)
metrics['Recall'].append(ensemble_recall)
metrics['F1 Score'].append(ensemble_f1)

# Initialize the best model tracking variables
best_model = None
best_accuracy = 0

# Loop through the original models dictionary and calculate metrics
for name, clf in models.items():  # Loop through individual models
    clf.fit(X_train, y_train)  # Ensure each classifier is trained
    y_pred_individual = clf.predict(X_test)

    individual_accuracy = accuracy_score(y_test, y_pred_individual)
    individual_precision = precision_score(y_test, y_pred_individual)
    individual_recall = recall_score(y_test, y_pred_individual)
    individual_f1 = f1_score(y_test, y_pred_individual)

    # Append the metrics for individual models
    metrics['Model'].append(name)
    metrics['Accuracy'].append(individual_accuracy)
    metrics['Precision'].append(individual_precision)
    metrics['Recall'].append(individual_recall)
    metrics['F1 Score'].append(individual_f1)

    # Track the best model based on accuracy
    if individual_accuracy > best_accuracy:
        best_accuracy = individual_accuracy
        best_model = name

# Create a DataFrame from the metrics dictionary for plotting
metrics_df = pd.DataFrame(metrics)

# Plot the comparison graph for Accuracy, Precision, Recall, and F1 Score
metrics_df.set_index('Model').plot(kind='bar', figsize=(12, 8), color=['skyblue', 'lightcoral','red','gold'], width=0.8)
plt.title('Model Performance Comparison: Accuracy, Precision, Recall, F1 Score')
plt.ylabel('Score')
plt.xlabel('Model')
plt.xticks(rotation=0)
plt.tight_layout()

# Add percentage labels on top of the bars (percentage representation of values)
for ax in plt.gca().patches:
    height = ax.get_height()
    plt.gca().annotate(f'{height * 100:.2f}%', xy=(ax.get_x() + ax.get_width() / 2., height),
                       ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),
                       textcoords='offset points')

plt.show()

# Print classification report for Soft Voting Ensemble
#print("\nClassification Report for Soft Voting Ensemble:")
#print(classification_report(y_test, y_pred))

"""### Regression Metrics (if Predicting Risk Scores or Severity Levels)"""

# Define the function to evaluate regression models
def evaluate_regression_models(X_train, y_train, X_test, y_test):
    models = {
        'Gradient Boosting': GradientBoostingClassifier(
            learning_rate=0.01,
            max_depth=5,
            min_samples_split=10,
            n_estimators=100,
            subsample=0.8
        ),

        'LightGBM': LGBMClassifier(
            n_estimators=200,
            learning_rate=0.01,
            num_leaves=31,
            max_depth=-1,
            subsample=0.8,
            colsample_bytree=1.0,
            random_state=42
        ),

        'Random Forest': RandomForestClassifier(
            bootstrap=False, max_depth=None, min_samples_leaf=1,
            min_samples_split=5, n_estimators=50, random_state=42
        )
    }


    # Initialize lists to store metrics
    model_names = []
    mae_scores = []
    mse_scores = []
    rmse_scores = []
    r2_scores = []

    # Evaluate each model
    for model_name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Calculate regression metrics
        mae = mean_absolute_error(y_test, y_pred)
        mse = mean_squared_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)

        # Append results
        model_names.append(model_name)
        mae_scores.append(mae)
        mse_scores.append(mse)
        rmse_scores.append(rmse)
        r2_scores.append(r2)

    # Create a DataFrame for comparison
    results_df = pd.DataFrame({
        'Model': model_names,
        'MAE': mae_scores,
        'MSE': mse_scores,
        'RMSE': rmse_scores,
        'R²': r2_scores
    })

    # Highlight the best model for each metric
    #print("\nRegression Model Comparison:")
    #print(results_df)

    # Identify the best model based on R² score
    best_model_idx = results_df['R²'].idxmax()
    best_model_name = results_df.iloc[best_model_idx]['Model']
    #print(f"\nThe best model based on R² score is: {best_model_name}")

    # Plot comparison
    results_df.set_index('Model', inplace=True)
    ax = results_df[['MAE', 'MSE', 'RMSE']].plot(
        kind='bar', figsize=(12, 6), title="Regression Model Metrics Comparison: Gradient Boosting vs LightGBM vs Random Forest", color=['skyblue', 'lightcoral', 'red']
    )
    plt.ylabel("Error Score")
    plt.xlabel("Model")
    plt.xticks(rotation=45)

    # Annotate values on top of the bars as percentages
    for p in ax.patches:
        value = f"{p.get_height() * 100:.2f}%"  # Convert to percentage
        ax.annotate(
            value,
            (p.get_x() + p.get_width() / 2., p.get_height()),
            ha='center', va='bottom', fontsize=10, color='black'
        )

    # Plot R² values separately
    plt.tight_layout()
    plt.show()

    # Print R² comparison
   # print("\nR² Scores:")
   # print(results_df['R²'])

    return results_df, best_model_name

# Example usage
# Assuming X_train, y_train, X_test, y_test are already defined for regression tasks:
results_df, best_model_name = evaluate_regression_models(X_train, y_train, X_test, y_test)

# Print the results DataFrame
#print(results_df)

"""#### Find the best model based on Accurcy ,ROC AUC ,log loss and soft voting"""

# Define the function to evaluate models based on all metrics and soft voting
def evaluate_models(X_train, y_train, X_test, y_test):
    models = {
        'Gradient Boosting': GradientBoostingClassifier(
            learning_rate=0.01,
            max_depth=5,
            min_samples_split=10,
            n_estimators=100,
            subsample=0.8
        ),

        'LightGBM': LGBMClassifier(
            n_estimators=200,
            learning_rate=0.01,
            num_leaves=31,
            max_depth=-1,
            subsample=0.8,
            colsample_bytree=1.0,
            random_state=42
        ),

        'Random Forest': RandomForestClassifier(
            bootstrap=False, max_depth=None, min_samples_leaf=1,
            min_samples_split=5, n_estimators=50, random_state=42
        )
    }


    # Initialize lists to store metrics
    model_names = []
    accuracy_scores = []
    roc_auc_scores = []
    log_loss_scores = []

    # Evaluate individual models
    for model_name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)

        # Calculate ROC AUC
        if hasattr(model, "predict_proba"):
            y_prob = model.predict_proba(X_test)[:, 1]
            fpr, tpr, _ = roc_curve(y_test, y_prob)
            roc_auc = auc(fpr, tpr)
            roc_auc_scores.append(roc_auc)
            log_loss_scores.append(log_loss(y_test, model.predict_proba(X_test)))
        else:
            roc_auc_scores.append(None)
            log_loss_scores.append(None)

        model_names.append(model_name)

    # Create VotingClassifier with soft voting
    voting_clf = VotingClassifier(estimators=list(models.items()), voting='soft')
    voting_clf.fit(X_train, y_train)
    y_pred_voting = voting_clf.predict(X_test)

    # Calculate metrics for the VotingClassifier
    accuracy_voting = accuracy_score(y_test, y_pred_voting)
    y_prob_voting = voting_clf.predict_proba(X_test)[:, 1]
    fpr_voting, tpr_voting, _ = roc_curve(y_test, y_prob_voting)
    roc_auc_voting = auc(fpr_voting, tpr_voting)
    log_loss_voting = log_loss(y_test, voting_clf.predict_proba(X_test))

    # Append VotingClassifier metrics to the lists
    model_names.append("Soft Voting Ensemble")
    accuracy_scores.append(accuracy_voting)
    roc_auc_scores.append(roc_auc_voting)
    log_loss_scores.append(log_loss_voting)

    # Create a DataFrame for comparison
    results_df = pd.DataFrame({
        'Model': model_names,
        'Accuracy': accuracy_scores,
        'ROC AUC': roc_auc_scores,
        'Log Loss': log_loss_scores
    })

    # Highlight the best model for each metric
    #print("\nModel Comparison:")
    #print(results_df)

    #print("\nBest Models:")
    #print(f"Best Model (Accuracy): {results_df.loc[results_df['Accuracy'].idxmax(), 'Model']}")
    #print(f"Best Model (ROC AUC): {results_df.loc[results_df['ROC AUC'].idxmax(), 'Model']}")
    #print(f"Best Model (Log Loss): {results_df.loc[results_df['Log Loss'].idxmin(), 'Model']}")
    #print(f"Soft Voting Ensemble Results:")
    #print(f"Accuracy: {accuracy_voting:.4f}, ROC AUC: {roc_auc_voting:.4f}, Log Loss: {log_loss_voting:.4f}")

    # Plot comparison
    results_df.set_index('Model', inplace=True)
    ax = results_df[['Accuracy', 'ROC AUC', 'Log Loss']].plot(
        kind='bar', figsize=(12, 6), title="Model Metrics Comparison: Gradient Boosting vs LightGBM vs Random Forest vs Soft Voting Ensemble", color=['skyblue', 'lightcoral', 'red']
    )
    plt.ylabel("Score")
    plt.xlabel("Model")
    plt.xticks(rotation=45)

    # Annotate values on top of the bars (show percentages)
    for p in ax.patches:
        value = f"{p.get_height() * 100:.2f}%" if not pd.isna(p.get_height()) else "N/A"
        ax.annotate(
            value,
            (p.get_x() + p.get_width() / 2., p.get_height()),
            ha='center', va='bottom', fontsize=10, color='black'
        )

    plt.tight_layout()
    plt.show()

    return results_df

# Example usage
# Assuming X_train, y_train, X_test, y_test are already defined:
results_df = evaluate_models(X_train, y_train, X_test, y_test)

# Print the results DataFrame
#print(results_df)

"""Introduction: In the realm of machine learning, choosing the best model for a specific task requires evaluating different performance metrics. In this comparison, three prominent models—Gradient Boosting, LightGBM, and Random Forest—are tested across several performance metrics: accuracy, ROC AUC, and log loss. Additionally, the performance of a Soft Voting Ensemble, which aggregates predictions from multiple models, is also considered. By examining these models based on multiple factors, we aim to identify the best-performing model for the given task.

Which Model is Best and Why? Based on the evaluation metrics, Gradient Boosting emerges as the best model when considering accuracy (0.910) and ROC AUC (0.9694). It consistently outperforms the other models in these areas, indicating that it is the most reliable model for classification tasks where prediction accuracy and distinguishing between classes matter. However, when it comes to log loss, Random Forest performs slightly better with a lower value of 0.2493, suggesting it may be more reliable in terms of probabilistic predictions. Despite this, Gradient Boosting’s overall performance in accuracy and ROC AUC makes it the top choice for most use cases.

Conclusions: While Gradient Boosting excels in both accuracy and ROC AUC, making it the best model in terms of general classification performance, Random Forest proves to be a strong contender in terms of minimizing log loss. The Soft Voting Ensemble, though effective, does not significantly outperform the best individual models across the board, with its accuracy and ROC AUC slightly trailing behind Gradient Boosting. Thus, for a well-rounded, high-performance model, Gradient Boosting is the best choice, offering a balanced combination of accuracy, ROC AUC, and solid generalization.

#### Recommendation:
##### Based on the comparison of various models, Gradient Boosting emerges as the best model for Ischemic Heart Disease (IHD) prediction. It demonstrated the highest accuracy (91.0%), ROC AUC (0.9694), and performed consistently well across key performance metrics, making it a reliable choice for the final model.

#### Save the model using joblib
"""

import joblib

# Assuming your trained Gradient Boosting model is named 'gb_model'
joblib.dump(gb_model, 'gradient_boosting_model.pkl')

print("Model saved successfully as 'gradient_boosting_model.pkl'")

"""#### Loading the Saved Model:"""

import joblib
import pandas as pd

# Load the saved Gradient Boosting model
loaded_model = joblib.load('gradient_boosting_model.pkl')

print("Model loaded successfully!")

import pandas as pd

# Example input data (you can replace this with the actual test data you want to predict)
input_data = pd.DataFrame({
    'Age (Years)': [60],
    'Stroke Status_Yes': [1],  # 1 for Yes, 0 for No
    'Weight (kg)': [70],
    'Random Blood Sugar (mg/dL)': [140],
    'Height (cm)': [175],
    'Systolic Blood Pressure (mmHg)': [130],
    'Diabetes Mellitus (DM) Status_Yes': [1],  # 1 for Yes, 0 for No
    'Hypertension (HTN) Status_Yes': [1],  # 1 for Yes, 0 for No
    'Dyslipidemia Status_Yes': [0],  # 1 for Yes, 0 for No
    'Diastolic Blood Pressure (mmHg)': [85],
    'Ischemic Heart Disease (IHD) Status_Yes': [0]  # 1 for Yes, 0 for No (target, not needed for prediction)
})

# Ensure that the data type is correct (e.g., boolean columns are in the right format)
input_data = input_data.astype({
    'Stroke Status_Yes': 'bool',
    'Diabetes Mellitus (DM) Status_Yes': 'bool',
    'Hypertension (HTN) Status_Yes': 'bool',
    'Dyslipidemia Status_Yes': 'bool',
    'Ischemic Heart Disease (IHD) Status_Yes': 'bool'  # Don't include this as a feature for prediction
})

from sklearn.ensemble import GradientBoostingClassifier

# Example of model training
gb_model = GradientBoostingClassifier()
gb_model.fit(X_train, y_train)  # Replace with your training data

# Save the model after fitting
import joblib
joblib.dump(gb_model, 'gradient_boosting_model.pkl')

import pandas as pd
import joblib

# Define all the feature columns (ensure these match the training features)
columns = [
    'Age (Years)', 'Stroke Status_Yes', 'Weight (kg)', 'Random Blood Sugar (mg/dL)',
    'Height (cm)', 'Systolic Blood Pressure (mmHg)', 'Diabetes Mellitus (DM) Status_Yes',
    'Hypertension (HTN) Status_Yes', 'Dyslipidemia Status_Yes',
    'Diastolic Blood Pressure (mmHg)', 'Feature_11', 'Feature_12', 'Feature_13', 'Feature_14', 'Feature_15',
    'Feature_16', 'Feature_17', 'Feature_18', 'Feature_19', 'Feature_20'
]

# Example: Make sure you have a data frame with the right number of features
input_data = pd.DataFrame([{
    'Age (Years)': 50, 'Stroke Status_Yes': 1, 'Weight (kg)': 70,
    'Random Blood Sugar (mg/dL)': 150, 'Height (cm)': 175, 'Systolic Blood Pressure (mmHg)': 130,
    'Diabetes Mellitus (DM) Status_Yes': 1, 'Hypertension (HTN) Status_Yes': 0,
    'Dyslipidemia Status_Yes': 0, 'Diastolic Blood Pressure (mmHg)': 85,
    # Add other features required by the model (these are just placeholders)
    'Feature_11': 0, 'Feature_12': 1, 'Feature_13': 0, 'Feature_14': 1, 'Feature_15': 0,
    'Feature_16': 1, 'Feature_17': 0, 'Feature_18': 1, 'Feature_19': 0, 'Feature_20': 1
}], columns=columns)

# Load the trained model
loaded_model = joblib.load('gradient_boosting_model.pkl')

# Make predictions on the new data
predicted_class = loaded_model.predict(input_data)  # Predict class (0 or 1)
predicted_proba = loaded_model.predict_proba(input_data)  # Get probabilities for each class

# Output the results
print(f"Predicted Class: {predicted_class[0]}")  # IHD Yes (1) or No (0)
print(f"Predicted Probabilities: {predicted_proba[0]}")  # Probability distribution for each class

"""##### Model Accuracy"""

# Print the accuracy and classification report
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the loaded model: {accuracy:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Optionally, you can also generate a confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)



